---
title: Vignette On How To Build Linear Regression In R Using Expenses data for Medical
  Treatment
author: "Ifesinachi Aroh"
date: "2024-09-28"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*Linear regression is a supervised machine learning algorithm that is used to predict the continuous variable. The algorithm assumes that the relation between the dependent variable(Y) and independent variables(X), is linear and is represented by a line of best fit. In this chapter, we will learn how to execute linear regression in R using some select functions and test its assumptions before we use it for a final prediction on test data*

# Overview – Linear Regression
In statistics, linear regression is used to model a relationship between a continuous dependent variable and one or more independent variables. The independent variable can be either categorical or numerical. The case when we have only **one independent variable** then it is called as **simple linear regression**. If we have **more than one independent variable**, then it is called as **multivariate regression**.


A mathematical representation of a linear regression model is as give below:

Y = β_0 + β_1X_1 + β_2X_2 + β_3X_3 + ….. + β_nX_n + error
In the above equation, β_0 coefficient **represents** intercept and β_i coefficient represents **slope**. Here we will be using a case study approach to help you understand the linear regression algorithm.

In the below case study, we will be using **expenses data for medical treatment** to predict the charges. Let us look at the top six observations of expenses.



```{r}
# Reading data
expenses <- read.csv("~/Downloads/expenses.csv")
# Print top 6 observations
head(expenses)
```

# Exploratory Data Analysis
Exploratory data analysis exercise is critical to any project related to Machine Learning. It is an approach to understand and summarize the main characteristics of a given data. Mostly, this involves slicing and dicing of data at different levels, and results are often presented with visual methods. If done correctly, it can reveal many aspects of the data, which will surely help you build better models.

Every dataset is different, and thus, it isn’t easy to list down steps one should perform as part of data exploration. However, the key to a successful EDA is to keep asking the questions which one believes helps in solving the business problem or put across all sorts of hypothesis and then testing them using appropriate statistical tests. Read

In other words, try to figure if there is a statistically significant relationship between the target and independent variables. What are the things which derive target variables?

Below are few things which we should consider exploring from the statistical point of view:

**1. Checking distribution of target variable** – First, you should always try to understand the nature of your target variable. To achieve this, we will be drawing a histogram with a density plot.

```{r,warning=FALSE, message=FALSE}
library(ggplot2)
# Building histogram
ggplot(data=expenses, aes(charges)) +
  geom_histogram(aes(y =..density..), fill = "orange") +
  geom_density()
```

The charges variable does not follow normal distribution. It is positively skewed.
A positively skewed response variable indicates that the data have a few extreme high values that distort the mean and stretch the right side of the distribution.

**2. Analyzing Summary Statistics** – Here, we will simply create summary statistics for all the variables to understand the behavior of all the independent variables. It will also provide information about missing values or outliers if any. For more information and functions which you can use [beginner’s guide to exploratory data analysis.](https://medium.com/@pp1222001/a-beginners-guide-to-exploratory-data-analysis-eda-8ac6fce9f598)

Both missing values and outliers are of concern for Machine Learning models as they tend to push the result towards extreme values.

```{r message = FALSE, warning=FALSE}
# loading psych package
library(psych)
psych::describe(expenses)

```
**Interpretation:**
**age:** The ages in the dataset range from 18 to 64 years, with a mean and median around 39 years, indicating a balanced distribution of ages around middle age.

**sex:** The mean of 1.51 suggests a fairly equal split between males and females (as 1 likely represents males and 2 represents females).

**bmi:** The average BMI is 30.66, with a relatively wide range (from 15.96 to 53.13). This indicates the dataset includes individuals across different body weight categories.

**children:** The number of children ranges from 0 to 5, with a mean of 1.09, suggesting most people have one child or none.

**smoker:** The mean value of 1.20 suggests most individuals are non-smokers (if 1 represents non-smokers and 2 represents smokers), but there is still a reasonable proportion of smokers in the dataset.

**region:** The average value of 2.52 (between 1 and 4) indicates a fairly balanced representation of geographic regions.

**charges:** The medical expenses (charges) have a large range (from 1,121.87 to 63,770.43), with the mean much higher than the median. This indicates the distribution of charges is positively skewed, meaning most people have relatively low charges, but a few people have extremely high charges (outliers).


**3. Checking Outliers Using Boxplots** – To learn more about outliers and how to identify, please read – [How To Identify & Treat Outliers Using Uni-variate Or Multivariate Methods.](https://towardsdatascience.com/detecting-and-treating-outliers-in-python-part-1-4ece5098b755). Here are using a boxplot for plotting the distribution of each numerical variable to check for outliers. 

If points lie beyond whispers, then we have outlier values present. For now, we are just going by univariate outlier analysis. But I encourage you to check for outliers at a multivariate level as well. If outliers are present, then you must either remove or do a proper treatment before moving forward.
```{r message = FALSE, warning=FALSE}
library(reshape)
meltData <- melt(expenses)
p <- ggplot(meltData, aes(factor(variable), value))
p + geom_boxplot() + facet_wrap(~variable, scale="free")
```
*Charges and bmi seem to have outliers.*


**4. Correlation Matrix Visualization –**

We will use corrgram package to visualize and analyze the correlation matrix. To learn more about how to check the significance of correlation and different ways of visualizing the correlation matrix, please read [Correlation In R – A Brief Introduction.](https://statsandr.com/blog/correlation-coefficient-and-correlation-test-in-r/) In theory, the correlation between the independent variables should be zero. In practice, we expect and are okay with weak to no correlation between independent variables.

We also expect that independent variables reflect a high correlation with the target variable.

```{r, message=FALSE}

require(corrgram)
corrgram(expenses, order=TRUE)
```

#### Key Takeaways:
* Strong Positive Correlations: `charges` with `age` and `bmi` (these are likely important predictors in the model).
* Weaker Correlations: Between `charges` and `children`, suggesting `children` might not be a strong predictor of `charges`.
* Low or No Correlation: Between `children`, `age`, and `bmi`, indicating that these factors are not strongly related to each other.

This visualization helps identify which variables are more likely to influence the target variable (charges) and which ones have weaker relationships. Based on this, we can focus on the variables with stronger correlations when building and refining our regression model.

# Training Regression Model
To build a linear regression, we will be using `lm()` function. The function takes two main arguments.

* **Formula** stating the dependent and independent variables separated by ~(tilder).
* The **dataset** name.
* There are other useful arguments and thus would request you to use `help(lm)` to read more from the documentation.

## Diving data into train and test subsets

The expenses data is divided into 70:30 split of train and test. The 70:30 split is the most common and is mostly used during the training phase. 70% of the data is used for training, and the rest 30% is for testing how good we were able to learn the data behavior.

```{r, eval=FALSE,}
install.packages("caret")
```

```{r, warning=FALSE, message=FALSE}
library(caret)
# Split data into train and test
set.seed(202)
index <- createDataPartition(expenses$charges, p = .70, list = FALSE)
train <- expenses[index, ]  # Training dataset (70%)
test <- expenses[-index, ]   # Testing dataset (30%)
# Checking the dim of train
dim(train)
```

### You can see we have 70% of the random observations in the training dataset which is 938 out of 1338.

# Building Model
```{r}
# Taining model
lmModel <- lm(charges ~ ., data = expenses)  # Using all other columns as predictors
# Printing the model object
print(lmModel)
```
## Interpreting Regression Coefficients
In the above output, **Intercept** represents that the minimum value of charges that will be received, if all the variables are constant or absent.

**Please note**

Intercept may not always make sense in business terms.

**Slope(represented by independent variables)** tells us about the rate of change that the Price variable will witness, with every one unit change in the independent variable. For example – if the numberof  **children** increases by one more unit, the **charges** for insurance will increase by 476.

# Validating Regression Coefficients and Models

We must ensure that the value of each beta coefficient is significant and has not come by chance. In R, the `lm` function runs a **one-sample t-test** against each beta coefficient to ensure that they are significant and have not come by chance. Similarly, we need to validate the overall model. Just like a one-sample t-test, `lm` function also generates three statistics, which help data scientists to validate the model. These statistics include **R-Square, Adjusted R-Square, and F-test**, also known as global testing.

To view these statistics, we need to pass the `lmModel` object to the `summary()` function.

```{r}
# Checking model statistics
summary(lmModel)
```

In the above output, **Pr(>|t|)** represents the p-value, which can be compared against the alpha value of 0.05 to ensure if the corresponding beta coefficient is significant or not. The `lm` function here lends a helping hand. All values in the output that have (.) period or (*) astric against the variable names indicates that these values are significant. Based upon this, we now know that all variables are statistically significant except
**sexmale** and **regionnorthwest**.

For overall model accuracy, let’s discuss statistics generated by `lm` function one by one.

**1. Multiple R-squared: 0.7509 –** The R-squared value is formally called a **coefficient of determination.** Here, 0.7509 indicates that the intercept, age, bmi, children, smokeryes, regionsoutheast, and regionsouthwest variables, when put together, are able to explain 75% of the variance in the charges variable. The value of R-squared lies between 0 to 1. In practical applications, if the R2 value is higher than 0.70, we consider it a good model.

**2. Adjusted R-squared: 0.7494 –** The Adjusted R-squared value tells if the addition of new information ( variable ) brings significant improvement to the model or not. So as of now, this value does not provide much information. However, the increase in the adjusted R-squared value with the addition of a new variable will indicate that the variable is useful and brings significant improvement to the model.

 > ## *A large difference between the R-Squared and Adjusted R-squared is not appreciated and generally indicates that multicollinearity exists within the data.* 

**3. F-statistic: 500.8 on 8 and 1329 DF,  p-value: < 2.2e-16 –** This line talks about the global testing of the model. The `lm` function runs an ANOVA test to check the significance of the overall model. Here the null hypothesis is that the model is not significant, and the alternative is that the model is significant. According to the p-values < 0.05, our model is significant.

Albeit, looking at these statistics is enough to take a call on the model significance. But there are other validation methods for linear regression that can be of help while deciding how good or bad the model is. Some of them are mentioned below:

**4. AIC and BIC values –** The AIC(Akaike’s information criterion, 1974) and BIC(Bayesian information criterion, 1978) are penalized-likelihood criteria. Both these measures use a “measure of fit + complexity penalty” to get the final values.

AIC = – 2 * ln(likelihood) + 2 * p

BIC = – 2 * ln(likelihood) + ln(N) * p

Here p = number of estimated parameters and N = sample size.

The AIC and BIC values can be used for choosing the best predictor subsets in regression and for comparing different models. When comparing different models, the model with minimum AIC and BIC values is considered the best model.

**Note**
AIC is likely to overfit the data, whereas BIC is susceptible to underfit the data.

*Key Differences Between AIC and BIC:*
*AIC: A more lenient penalty for model complexity. It is commonly used for model selection when you're less concerned about overfitting or when you want a more flexible model.
*BIC: Imposes a stronger penalty for the number of parameters, making it more conservative than AIC. BIC is often preferred when you want to avoid overfitting or when working with large sample sizes.


```{r}
# Using AIC function
AIC(lmModel)
# Using BIC function
BIC(lmModel)  
```

These values by themselves don’t provide much insight unless compared with other models. You would compute the AIC and BIC for several competing models, and then select the model with the lowest AIC or BIC


**5. Root Mean Square Error(RMSE) – ** By comparing the RMSE statistics of different models, we can decide which is a better model. The model with the lower RMSE value is considered a better model. There are other similar functions like MAE, MAPE, MSE, and so on that can be used. These functions can be found in `Metrics` R Package. These functions take majorly two arguments: One is the actual value and second, predicted values. So let’s see how we can get these values. The actuals can be 100% found from the original dataset or the training data in our case. However, to find the fitted values, we need to explore the model object.

```{r}
# Checking model object for actual and predicted values
names(lmModel)
```

The above vector presents the names of the object that constitute the model object. Here, fitted values are the predicted values. Now, we will use these values to generate the **rmse** values.

```{r, message=FALSE,warning=FALSE}
library(Metrics)
rmse(actual = train$charges, predicted = lmModel$fitted.values)
```

*Interpretation:*

* The RMSE value represents the average error between the model’s predictions and the actual values. Here, an RMSE of 15750.84 (this value might vary if you reproduce this model or re-run this chunk because the splitting of our training and test data happens randomly, if you want this value to be constant, set a seed value before splitting) means that, on average, the predictions are off by about 15750 units from the actual values of charges.

* Lower RMSE values indicate that the model's predictions are closer to the actual values, implying a better fit.

* Higher RMSE values indicate that the model has larger prediction errors, suggesting the model might not be performing well.


# Checking Assumptions of Linear Regression

Linear regression is parametric, which means the algorithm makes some assumptions about the data. A linear regression model is only deemed fit is these assumptions are met. There are about four assumptions and are mentioned below. If the model fails to meet these assumptions, then we simply cannot use this model

**1. Errors should follow normal distribution –** This can be checked by drawing a histogram of residuals or by using `plot()` function. The `plot` function creates 4 different charts. One of which is an NPP plot. The chart confirms if the errors follow a normal distribution or not.

## Generating histogram

```{r, warning=FALSE, message=FALSE}
# Histogram to check the distribution of errors
hist(lmModel$residuals, color = "grey",)
```

#### The above histogram shows the errors are normally distrubuted but not perfectly (Which underscores the fact that theoretical assumptions are ideal but not always 100% ascertained in real applications). The error distribution is roughly centered around zero but exhibits a positive skew with a long right tail. This indicates that while the model performs reasonably well in most cases, it tends to underpredict the target variable in some instances, leading to significant positive errors.

## Generating NPP plot
We except the points to be very close to the dotted line in an NPP plot. Points being close to the line means that errors follow a normal distribution.

```{r}
plot(lmModel)
```

#### Interpretation

**a. Residuals vs Fitted:**
There is evidence of non-linearity in the data, which indicates that the linear model may not be fully appropriate, and you may need to explore non-linear relationships or interactions among variables.

**b. Normal Q-Q Plot:**
The residuals are approximately normally distributed, but there are deviations at the extremes (outliers), which could affect the validity of the linear regression assumptions.

**c. Scale-Location Plot:**
The model exhibits heteroscedasticity, meaning that the variance of the residuals changes with the level of the fitted values. This violates one of the key assumptions of linear regression, and transformations or alternative models might be necessary.

**d. Residuals vs Leverage Plot:**
The plot indicates that there are some influential data points with both high residuals and high leverage. These points should be carefully examined and possibly removed or handled differently, as they could be affecting the model's performance.

**2. There should be no heteroscedasticity –** This means that the variance of error terms should be constant. We shall not see any patterns when we draw a plot between residuals and fitted values. And the mean line should be close to Zero.

## Generating the scatterplot between residuals and fitted values

```{r, eval=FALSE}
# Using plot function
plot(lmModel)
```
**Using the Scale-Location Plot above to check for heteroscedasticity:**

* **Purpose:** This plot checks for homoscedasticity (constant variance of residuals). It shows the standardized residuals against the fitted values.
* **Ideal pattern:** The points should be randomly scattered with no clear pattern, and the red line should be relatively flat, indicating constant variance.
* **Observed pattern:**
  * There is a clear funnel-like shape, where the spread of the residuals increases as the fitted values increase (more variability in larger fitted values).
  * The red line curves upward slightly, further indicating non-constant variance (heteroscedasticity).
  
The model exhibits heteroscedasticity, meaning that the variance of the residuals changes with the level of the fitted values. 

**3. There should be no multicollinearity –** The linear model assumes that the predictor variables do not correlate with each other. If they exhibit high correlation, it is a problem and is called **multicollinearity.** A variation inflation factor test can help check for the multicollinearity assumption.

VIF = 1/(1-R2)

VIF is an iterative process. The function will remove one variable at a time, which is cause for multicollinearity and repeats the process until all problem causing variables are removed. So, finally, we are left with the list of variables that have no or very weak correlation between them.

```{r, cache=TRUE, warning=FALSE,message=FALSE}
library(car)
vif (lmModel)

```
##### Interpreting VIF Values:
* A VIF of 1 means there is no correlation between the given predictor and any others.
* VIF between 1 and 5 suggests moderate correlation that’s usually acceptable.
* VIF > 5 indicates high correlation and suggests that multicollinearity may be problematic.

*There is no multicollinearity problem in the dataset. Generally, VIF values which are greater than 5 or 7 are the cause of multicollinearity.*


**4. There should be no auto serial correlation –** The autocorrelation means that error terms should not be correlated with each other. To check this, we can run the Durbin-Watson test(dw test). The test returns a value between 0 and 4. If the value is two, we say there is no auto serial correlation. However, a value higher than 2 represents (-) ve correlation and value lower than 2 represents (+) ve correlation.

```{r, warning=FALSE,message=FALSE}
library("lmtest")
dwtest(lmModel)
```
*We got a value of 2.0884 which suggests that there is no auto serial correlation.*

**Our model does not fully meet all the four assumptions of linear regression, some tranformation might be required to make it more efficient.**

## Predicting Dependent Variable(Y) in Test Dataset

We test the model performance on test data set to ensure that our model is stable, and we get the same or closer enough results to use this trained model to predict and forecast future values of dependent variables. To `predict`, we use predict function, and then we generate R-Squared value to see if we get the same result as we got in the training dataset or not.

```{r}
# Predicting charges in test dataset
test$Predictedcharges <- predict(lmModel, test)

# Priting top 6 rows of actual and predicted charges
head(test[ , c("charges", "Predictedcharges")])

```

## Generating R-Squared Value for the test dataset

We are using a user-defined formula to generate the R-Squared value here.

```{r}
actual <- test$charges
predicted <- test$Predictedcharges
rss <- sum((predicted - actual) ^ 2)
tss <- sum((predicted - mean(actual)) ^ 2)
rsq <- 1 - rss/tss
rsq
```

*This model is fairly good, but there is room for improvement*

*In the test dataset, we got an accuracy of 0.7274106 (this value might vary if you reproduce this model or re-run this chunk because the splitting of our training and test data happens randomly, if you want this value to be constant, set a seed value before splitting) and in training data set, we got an accuracy of 0.7509*

In this document, We learnt many things related to linear regression from a practical and theoretical point of view. We learned when to use linear regression, how to use it, how to check the assumptions of linear regression, how to predict the target variable in test dataset using trained model object, and we also learned how to validate the linear regression model using different statistical methods. 